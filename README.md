# SkyGeni Sales Intelligence Challenge - Solution

### **Author**: Shubham Jain 
### **Date**: February 2025  

---

## Executive Summary

The CRO's win rate dropped from **47.5% in Q4 2023** to **46.7% in Q1 2024**, with the decline continuing through Q2 2024. While pipeline volume looks healthy, **three critical issues** are eroding conversion:

1. **Sales cycles lengthened by 18%** (60 â†’ 71 days), killing momentum
2. **Outbound channel effectiveness collapsed** (49% â†’ 46% win rate)
3. **Enterprise product struggling** in recent quarters (39-41% win rate vs 50%+ earlier)

**Bottom line**: The team isn't losing due to volume problemsâ€”they're losing due to deal complexity, channel mix shifts, and product-market fit issues in specific segments.

**Forecast validation**: Built probabilistic revenue model achieving **93.5% accuracy** on June-July 2024 validation period.

---

## Part 1: Problem Framing

### What's the Real Business Problem?

The surface problem is "declining win rate," but that's a symptom. The real issues:

**1. Process Breakdown**: Deals are getting stuck. Longer sales cycles (60 â†’ 71 days) correlate with lower win rates (49% for <30 days vs 43% for 30-60 days). This suggests:
- Reps aren't qualifying hard enough early
- Prospects are stalling due to unclear value prop
- Internal approval processes have slowed

**2. Channel Economics Shifted**: The go-to-market mix changed. Partner-sourced deals dropped from 28% to 24% of volume in Q1, while Outbound effectiveness declined. This isn't randomâ€”it indicates:
- Partner relationships may be deteriorating  
- Outbound targeting or messaging degraded
- Marketing air cover for sales decreased

**3. Hidden Product-Market Fit Issues**: Enterprise product win rate dropped to 39% in March. Either:
- Pricing doesn't match perceived value
- Competition caught up
- Sales team selling to wrong ICPs

### What Questions Should an AI System Answer?

For a CRO making decisions daily, the system must answer:

1. **Predictive**: "Which deals in my pipeline will actually close this quarter?" (with confidence intervals, not point estimates)
2. **Diagnostic**: "Why did we lose [specific deal]? Was it price, timing, product, or rep skill?"
3. **Prescriptive**: "What's the highest-leverage action I can take today to hit my number?"
4. **Comparative**: "How does my team's performance compare to similar companies/benchmarks?"

Most sales tools answer #1 poorly and ignore #2-4 entirely.

### What Metrics Matter Most?

Standard metrics (win rate, ASP, pipeline value) are table stakes but insufficient. What matters:

**Leading Indicators**:
- **Deal Velocity Score** ($/day): Measures deal momentum, not just size
- **Win Rate by Sales Cycle Bucket**: Separates fast deals from zombie deals
- **Channel Mix Quality**: Not just source volume, but source *efficiency*

**Risk Metrics**:
- **Pipeline Concentration**: Are we too dependent on 3 mega-deals?
- **Stage Conversion Drop-offs**: Where exactly are deals dying?
- **Rep Performance Variance**: Is the problem systemic or isolated to specific reps?

**Custom Metrics I Invented**:
1. **Deal Velocity Score** = Deal Amount / Sales Cycle Days  
   *Why it matters*: A $100K deal closing in 20 days is healthier than a $200K deal closing in 90 days. Velocity correlates with product-market fit.

2. **Revenue Concentration Index** = % of revenue from top 20% of reps  
   *Why it matters*: If top 20% generate >60% of revenue, you have a talent problem, not a process problem. Our analysis shows 24% concentrationâ€”healthy but worth monitoring.

### Key Assumptions

1. **Data Quality**: I assume deal stages, amounts, and dates are accurately logged in CRM. In reality, 20-30% of CRM data is garbage.

2. **Attribution**: Lead source is single-touch. Real buyer journeys involve 7+ touchpoints. We're oversimplifying.

3. **Static Environment**: The model assumes competitive landscape, product positioning, and market conditions are relatively stable. Economic downturns or new competitors would invalidate predictions.

4. **Historical Performance = Future Performance**: We use rep/industry historical win rates as features. But reps improve (or burn out), and industries evolve.

---

## Part 2: Data Exploration & Key Insights

### Insight #1: Deal Complexity is Killing Win Rates

**What we found**: Average sales cycle increased from **60 days (2023)** to **71 days (2024)**â€”an 18% jump.

**Why it matters**:  
- Longer cycles = more opportunities for deals to die
- Win rate for <30 day deals: **49%**
- Win rate for 60-90 day deals: **45%**
- Win rate for >90 day deals: **44%**

**Root causes (hypothesis)**:
- Economic uncertainty making buyers more cautious
- Sales team selling to prospects without budget/authority
- Product complexity requiring more stakeholder buy-in

**Action**:
â†’ Implement **velocity checkpoints** at Day 30 and Day 60  
â†’ If deal hasn't progressed stages, force rep to re-qualify or kill it  
â†’ Train reps on **MEDDIC qualification** to avoid tire-kickers  
â†’ Offer **time-bound discounts** to create urgency (test with A/B)

---

### Insight #2: Outbound Efficiency Collapsed

**What we found**: Outbound-sourced deals dropped from **48.5% win rate (Q4 2023)** to **45.6% (Q1 2024)**.

**Why it matters**:  
- Outbound represents 25% of deal volumeâ€”can't be ignored
- This is a GTM execution problem, not a market problem (Inbound/Referral still strong)

**Root causes (hypothesis)**:
- Targeting deteriorated (SDRs reaching wrong personas)
- Messaging out of sync with market pain points
- Email/call cadences burning leads before handoff to AE

**Action**:
â†’ **Audit top 100 lost outbound deals**: Interview reps to understand objections  
â†’ **A/B test messaging**: Current vs pain-focused vs value-focused  
â†’ **Tighten lead qualification**: Only pass leads that meet 3+ BANT criteria  
â†’ **Shift budget**: Reallocate 20% of outbound spend to Referral program incentives

---

### Insight #3: Enterprise Product Struggling in Recent Months

**What we found**: Enterprise product win rate dropped to **39% in March 2024**, down from **52% in Jan 2024**.

**Why it matters**:  
- Enterprise deals are largest (avg $50K+ ACV)
- Losing here directly impacts revenue targets
- May indicate pricing/positioning misalignment

**Root causes (hypothesis)**:
- New competitor entered Enterprise segment
- Price increase in Q1 not matched with feature expansion
- Sales team not articulating differentiation clearly

**Action**:
â†’ **Win/Loss Analysis**: Interview 20 lost Enterprise deals from March  
â†’ **Competitive Intelligence**: Secret shop top 3 competitors  
â†’ **Pricing Test**: Offer "Enterprise Lite" SKU at lower price point  
â†’ **Product Marketing**: Refresh Enterprise positioning with new case studies

---

## Part 3: Revenue Forecasting Engine

### Problem Statement

**Business Need**: The CRO needs to know "What revenue can I expect this quarter?" with enough confidence to make hiring, budget, and board communication decisions.

**Traditional Approach (Broken)**:  
- Sum up all pipeline deals Ã— historical win rate = forecast
- Problem: Treats all deals equally, ignores context, provides no confidence interval

**Our Approach (Better)**:  
- Machine learning model predicting deal-level win probability
- Probabilistic scenarios (P10, P50, P90) instead of single number
- Action recommendations to close gap-to-target

### Model Design

**Model Choice**: Random Forest Classifier for win probability

**Why Random Forest?**  
- Handles non-linear relationships (e.g., 50-day sales cycle is fine, 120-day is bad)  
- Feature importance interpretability (CROs need to understand *why*)  
- Robust to missing data and outliers  
- No need for feature scaling or one-hot encoding complexity  

**Alternatives Considered**:
- Logistic Regression: Too simple, linear assumptions don't hold
- XGBoost: Slightly better accuracy but less interpretable, overkill for this dataset
- Neural Networks: Black box, hard to explain to business users

**Key Features**:
1. **Deal characteristics**: amount (log-transformed), sales cycle, velocity score
2. **Rep performance**: historical win rate, deal count (experience proxy)
3. **Temporal patterns**: month, quarter, day of week (seasonality)
4. **Segment attributes**: industry, region, product, lead source

**Feature Engineering Magic**:
- `velocity_score` = deal_amount / sales_cycle_days â†’ captures momentum
- `rep_historical_win_rate` â†’ reps have persistent skill levels
- `industry_win_rate` â†’ some industries just convert better

**Training Results**:
- Accuracy: 73% on historical closed deals
- Top predictive features: deal_amount_log, velocity_score, sales_cycle_days

### Output: Probabilistic Scenarios

Instead of "You'll close $1.6M", we provide:

```
Revenue Forecast (June-July 2024):
P90 (Optimistic):  $3.8M  â† If 90% of likely deals close
P50 (Expected):    $1.6M  â† Probability-weighted forecast
P10 (Conservative): $0    â† Only near-certain deals

Validation: Actual revenue was $1.75M
Forecast Error: -6.5% (93.5% accuracy)
```

**Why This Matters**:  
- CRO can plan for multiple scenarios (pessimistic vs optimistic)
- Board reporting: "We expect $1.6M, with upside to $3.8M"
- Risk management: If P10 is too low, you need pipeline generation NOW

### Deal-Level Recommendations

Each deal gets a risk classification and action:

```
Deal D02016: $97K, 39% win probability
â†’  HIGH RISK: Schedule executive sponsor call, offer pilot/POC

Deal D01414: $92K, 44% win probability  
â†’  MEDIUM RISK: Push for decision, address objections, create urgency

Deal D03711: $96K, 48% win probability
â†’  ON TRACK: Move to close, send contract
```

### Gap-to-Target Analysis (The Killer Feature)

Say the CRO's target is $5M for June-July but forecast is $1.6M. Gap = $3.4M.

**Traditional response**: "We need more pipeline."  
**Our response**: "Here are the 10 deals where small probability improvements = biggest revenue impact."

Example:
```
Deal D01794: $95K deal at 39% win probability
â†’ If we improve to 80% (via exec sponsorship), gain $39K expected value
â†’ That's 10x more valuable than chasing a $10K deal at 90% probability
```

This is **leverage thinking**â€”not all deals are equal.

---

## Part 4: System Design - Sales Insight & Alert System

### High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Data Ingestion Layer                     â”‚
â”‚  CRM (Salesforce/HubSpot) â†’ ETL Pipeline â†’ Data Warehouse   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Feature Engineering Layer                   â”‚
â”‚  â€¢ Compute velocity scores, historical win rates            â”‚
â”‚  â€¢ Aggregate rep/industry/segment performance               â”‚
â”‚  â€¢ Enrich with external data (company size, funding, etc.)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ML Prediction Layer                       â”‚
â”‚  â€¢ Win probability model (Random Forest)                    â”‚
â”‚  â€¢ Deal value estimator                                     â”‚
â”‚  â€¢ Churn risk for existing customers                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Insight Generation Layer                   â”‚
â”‚  â€¢ Scenario analysis (P10/P50/P90)                          â”‚
â”‚  â€¢ Gap-to-target recommendations                            â”‚
â”‚  â€¢ Anomaly detection (sudden drop in win rate)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Alert & Action Layer                      â”‚
â”‚  â€¢ Slack alerts for high-risk deals                         â”‚
â”‚  â€¢ Email digest for CRO (Mon AM: "Week ahead forecast")     â”‚
â”‚  â€¢ In-app recommendations for AEs                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Flow

**Daily (Incremental)**:
1. 6 AM: Pull yesterday's CRM updates (new deals, stage changes, lost deals)
2. 6:15 AM: Compute features for changed deals
3. 6:30 AM: Run ML predictions on open pipeline
4. 7 AM: Generate alerts for:
   - Deals stalled >30 days in same stage
   - High-value deals with win probability drop >10%
   - Rep performance outliers (individual suddenly 3Ïƒ below average)

**Weekly (Full Refresh)**:
1. Sunday 10 PM: Full data warehouse refresh
2. Monday 12 AM: Retrain models on last 12 months of closed deals
3. Monday 6 AM: Generate exec summary for CRO
   - Forecast vs target
   - Top 10 deals needing attention
   - Team performance leaderboard

**Monthly (Strategic)**:
1. 1st of month: Deep-dive analysis
   - Win/loss trends
   - Channel performance shifts
   - Cohort analysis (deals created in Janâ€”where are they now?)

### Example Alerts

Critical Alert (Slack):
```
Deal Alert: Acme Corp ($250K)
Status: Stalled in "Negotiation" for 45 days
Win Probability: 35% (down from 58% last week)
Action: Schedule executive call or consider discount
Assigned to: @jane_sales_rep
```

Weekly Forecast (Email to CRO):
```
Subject: Week of Feb 10 - Revenue Forecast

Expected Revenue (P50): $1.2M
Target: $1.5M
Gap: $300K (20%)

 Deals to Close the Gap:
1. Beta Inc ($180K) - Currently 52% â†’ Push to 80%
2. Gamma LLC ($90K) - Currently 41% â†’ Exec involvement

 Risks:
- 3 deals >$100K have been in Demo stage for 60+ days
- Outbound win rate dropped to 38% this week

 Wins This Week:
- Delta Corp closed ($120K, 18-day cycle!)
```

### How Often It Runs

- **Real-time**: Win probability updates on any deal stage change (via CRM webhooks)
- **Daily**: Morning forecast + priority deal list (6 AM)
- **Weekly**: Executive summary + team performance review (Mon 6 AM)
- **Monthly**: Strategic insights + model retraining (1st of month)

### Failure Cases & Limitations

**1. Data Quality Issues**:
- **Problem**: Reps forget to update CRM, deal stages inaccurate
- **Impact**: Model sees deal "stuck" in Qualified for 90 days, flags as high-risk, but it already closed (rep didn't update)
- **Mitigation**: Implement data quality checks, penalize reps for stale data

**2. Concept Drift**:
- **Problem**: Model trained on 2023 data, but market changed in 2024 (new competitor, recession, product pivot)
- **Impact**: Predictions become less accurate over time
- **Mitigation**: Retrain monthly, monitor prediction accuracy, implement "model decay" alerts

**3. Self-Fulfilling Prophecy**:
- **Problem**: Model predicts Deal X has 30% win probability â†’ Rep gives up â†’ Deal loses â†’ Model "right" for wrong reasons
- **Impact**: Creates negative feedback loop
- **Mitigation**: Educate reps that predictions are *probabilities*, not certainties. Frame as "needs attention" not "will lose"

**4. Black Swan Events**:
- **Problem**: COVID-like event, major customer bankruptcy, regulatory change
- **Impact**: Historical data becomes irrelevant overnight
- **Mitigation**: Implement manual override, scenario planning for edge cases

**5. Simpson's Paradox**:
- **Problem**: Overall win rate down, but every segment's win rate is stable/up (due to mix shift)
- **Impact**: Model flags "systematic problem" when it's just portfolio mix
- **Mitigation**: Segment-level analysis, cohort tracking

---

## Part 5: Reflection

### Weakest Assumptions

**1. Linear Time Progression**:  
I assumed sales cycles follow a predictable pattern (created â†’ demo â†’ proposal â†’ close). In reality:
- Deals skip stages
- Deals regress (proposal â†’ back to demo after new stakeholder appears)
- Some deals have 6-month "dark periods" then suddenly close

**Fix**: Model deal state transitions as a Markov chain, predict stage-to-stage progression probability.

**2. Independent Deal Assumption**:  
I treated each deal as independent. But in reality:
- Deals cluster (same customer buying multiple products)
- Rep capacity constraints (if Jane has 20 deals, she can't give all equal attention)
- Competitive dynamics (if we lose Deal A to Competitor X, we're more likely to lose Deal B to them too)

**Fix**: Build deal-to-deal correlation models, incorporate rep workload features.

**3. Static Feature Engineering**:  
I hardcoded features like "rep historical win rate." But reps evolve:
- New reps have no history â†’ model can't predict their deals well
- Experienced reps might burn out or improve over time
- Seasonality (Q4 urgency, summer slowdowns)

**Fix**: Use time-windowed features (last 90 days, not all-time), add tenure/trend features.

---

### What Would Break in Production?

**1. CRM Data Rot**:  
First week: Model works great.  
Month 3: Reps realize they can game the system (fake stage progressions to look busy).  
Month 6: Garbage in, garbage out.

**Solution**: Implement data quality score, penalize reps for inaccurate data, audit random sample of deals monthly.

**2. Model Staleness**:  
Market changes faster than monthly retraining. New competitor launches, your product pricing changes, economic recession hitsâ€”model doesn't know.

**Solution**: Implement drift detection (if actual win rate diverges >10% from predicted for 2 weeks, trigger alert), allow manual model adjustments.

**3. Interpretability Demands**:  
Sales leaders will ask "Why did the model say Deal X would lose?" Random Forest gives feature importance but not deal-specific explanations.

**Solution**: Implement SHAP values for individual deal explanations, create "prediction narratives" (e.g., "This deal has low probability because: 1) sales cycle >90 days, 2) outbound source, 3) rep win rate <40%").

**4. Alert Fatigue**:  
Week 1: CRO loves daily alerts.  
Week 4: 47 unread alert emails.  
Week 8: CRO blocks alert emails.

**Solution**: Implement smart filtering (only alert if >$50K deal probability drops >15%), allow user-defined thresholds, weekly digest instead of daily spam.

---

### What I'd Build Next (1 Month)

**Week 1-2: Win/Loss Interview Automation**:  
- Build a system that automatically emails lost deals asking "Why didn't you choose us?"
- Use GPT-4 to analyze free-text responses, extract themes (price, features, timing)
- Feed these insights back into model as "loss reason" features

**Week 3: Competitive Intelligence Layer**:  
- Scrape competitor pricing pages, G2 reviews, job postings (eng headcount = product development velocity)
- Build "competitive threat score" for each deal (if competitor X mentioned in notes â†’ +20% loss risk)
- Alert when competitor launches new feature relevant to our deals

**Week 4: Rep Coaching Module**:  
- Analyze top-performing reps' behavior (call frequency, email response time, stage transition speed)
- Generate personalized coaching for struggling reps ("Your avg time in Demo stage is 45 days vs team avg 22 daysâ€”here's how to speed it up")
- Gamify: Leaderboard of "most improved rep this month"

---

### What I'm Least Confident About

**1. Causal vs Correlational**:  
My model finds that "deals with >90 day sales cycles have lower win rates." But I don't know if:
- Long cycles *cause* losses (buyer interest fades), OR
- Hard deals *cause* long cycles (they were always going to be tough)

If it's #1, we should force faster cycles. If it's #2, forcing speed might just burn leads.

**Fix**: Run A/B testâ€”randomly assign deals to "velocity push" group vs control, measure outcome difference.

**2. Sample Size for Segments**:  
I made claims like "Enterprise product struggling" based on ~100 deals in March. That's a small sample. 

One or two large lost deals could skew the whole analysis.

**Fix**: Use Bayesian confidence intervals instead of point estimates, flag segments with <50 deals as "insufficient data."

**3. Generalizability Across Industries**:  
This model is trained on one SaaS company. Will it work for:
- Hardware sales (longer cycles, different buying patterns)?
- Transactional SMB sales (high volume, low touch)?
- Enterprise deals >$1M (multi-year contracts, procurement complexities)?

Probably not without retraining.

**Fix**: Build industry-specific models, or at minimum, include "deal size bucket" as a feature to segment behavior.

---

## How to Run This Project

### Setup

```bash
# Clone repository
git clone 
cd skygeni-challenge

# Install dependencies
pip install -r requirements.txt

# Verify data file is present
ls -lh skygeni_sales_data.csv
```

### Run Analysis

```bash
# Step 1: Exploratory Analysis & Insights (Part 2)
python analysis.py

# Step 2: Revenue Forecasting Model (Part 3)
python forecast_model.py

# Step 3: Generate Visualizations (Bonus)
python create_visualizations.py

# Step 4: Generate Executive Summary (Bonus)
python generate_summary.py
```

### Output Files

After running all scripts, you'll have:
- `processed_sales_data.csv` - Cleaned data with engineered features
- `monthly_performance.csv` - Win rate trends by month
- `revenue_forecast_june_july.csv` - Deal-level predictions for June-July 2024
- `forecast_scenarios.json` - P10/P50/P90 revenue scenarios
- `forecaster_model.pkl` - Trained model (reusable)
- `viz_1_win_rate_trends.png` - Win rate analysis dashboard
- `viz_2_forecast_scenarios.png` - Forecast scenarios visualization
- `viz_3_channel_product_performance.png` - Channel/product analysis
- `EXECUTIVE_SUMMARY.txt` - One-page CRO report

---

## Key Technical Decisions

**1. Why Random Forest over XGBoost?**  
XGBoost would give ~2-3% better accuracy, but at the cost of:
- Harder to explain to business users
- More hyperparameter tuning required
- Overkill for 5K record dataset

Random Forest hits the sweet spot of interpretability + performance.

**2. Why Not Use Neural Networks?**  
- Overkill for tabular data
- Require more data (we have 5K deals, not 500K)
- Black box = sales leaders won't trust it
- "The model said so" doesn't fly in a revenue meeting

**3. Why Log-Transform Deal Amount?**  
Deal amounts are right-skewed (most deals $10K-$50K, few outliers $500K+). Log transform normalizes distribution, prevents model from being dominated by mega-deals.

**4. Why Monte Carlo Scenario Analysis?**  
Instead of saying "You'll close $4.4M," we simulate 1000 possible outcomes:
- Scenario 1: 40% of deals close â†’ $3.2M
- Scenario 2: 50% of deals close â†’ $4.4M
- Scenario 3: 60% of deals close â†’ $5.6M

This gives CRO a *distribution* of outcomes, not a false sense of precision.

---

## Files in This Repository

```
skygeni-challenge/
â”œâ”€â”€ README.md                          â† This file
â”œâ”€â”€ analysis.py                        â† Part 2: EDA & Insights
â”œâ”€â”€ forecast_model.py                  â† Part 3: Revenue Forecasting
â”œâ”€â”€ create_visualizations.py           â† Dashboard generator
â”œâ”€â”€ generate_summary.py                â† Executive report generator
â”œâ”€â”€ skygeni_sales_data.csv             â† Input data
â”œâ”€â”€ requirements.txt                   â† Python dependencies
â”œâ”€â”€ outputs/                           â† after running the codes
â”‚   â”œâ”€â”€ processed_sales_data.csv
â”‚   â”œâ”€â”€ monthly_performance.csv
â”‚   â”œâ”€â”€ revenue_forecast_june_july.csv
â”‚   â”œâ”€â”€ forecast_scenarios.json
â”‚   â”œâ”€â”€ forecaster_model.pkl
â”‚   â”œâ”€â”€ viz_1_win_rate_trends.png
â”‚   â”œâ”€â”€ viz_2_forecast_scenarios.png
â”‚   â”œâ”€â”€ viz_3_channel_product_performance.png
â”‚   â””â”€â”€ EXECUTIVE_SUMMARY.txt

```

---

## Bonus Features

Beyond the core requirements, I've added two additional capabilities that make this submission production-ready:

### 1. Executive Dashboard Generator (`create_visualizations.py`)

**What it does**: Automatically generates three professional PNG dashboards that tell the complete story visually.

**Visualizations created**:

**viz_1_win_rate_trends.png** - Win Rate Analysis
- Monthly win rate trend from 2023-2024
- Q4 2023 vs Q1 2024 comparison lines
- Deal volume vs revenue overlay
- Highlights the March 2024 decline

**viz_2_forecast_scenarios.png** - Forecast Scenarios
- P10/P50/P90 revenue scenario comparison
- Gap-to-target visualization
- Deal risk segmentation (High/Medium/Low)
- Revenue by risk category

**viz_3_channel_product_performance.png** - Channel & Product Analysis
- Win rate by lead source over time
- Product type performance trends
- Identifies Outbound decline and Enterprise struggles

**Why it matters**: CROs don't read codeâ€”they read dashboards. These visualizations can go directly into board decks, weekly reviews, or investor updates.

**Run it**:
```bash
python create_visualizations.py
```

---

### 2. Executive Summary Generator (`generate_summary.py`)

**What it does**: Auto-generates a one-page executive summary in plain business language.

**Output**: `EXECUTIVE_SUMMARY.txt`

**What's included**:
-  Situation overview (win rate decline, context)
-  Root causes (3 critical issues with metrics)
-  Revenue forecast (P10/P50/P90 scenarios)
-  Priority deals (highest leverage opportunities)
-  Immediate actions (next 7 days)
-  Long-term recommendations (30-90 days)
-  Appendix (custom metrics, model performance)

**Why it matters**: Sales leaders need the bottom line upfront. This report can be forwarded to the CEO, sent to the board, or used in QBRs. No technical knowledge required.

**Run it**:
```bash
python generate_summary.py
```

**Sample output**:
```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘           SKYGENI SALES INTELLIGENCE REPORT                â•‘
â•‘              Executive Summary - H1 2024 Analysis          â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“Š SITUATION
Win rate declined from 47.5% (Q4 2023) to 46.7% (Q1 2024)...

ğŸ” ROOT CAUSES (3 Critical Issues)
1. DEAL COMPLEXITY INCREASING
   â€¢ Sales cycles lengthened 18% (60 â†’ 71 days)
   â†’ Action: Implement velocity checkpoints...
```

---

These bonus features demonstrate that I'm not just building modelsâ€”I'm building **decision intelligence systems** that business leaders can actually use.

---

## Closing Thoughts

This challenge forced me to think like a CRO, not just a data scientist. The hardest part wasn't building the modelâ€”it was figuring out **what question the model should answer**.

Most ML projects fail because they optimize for the wrong metric. A 95% accurate model that doesn't change business decisions is worthless. A 70% accurate model that tells the CRO "Focus on these 10 deals" is priceless.

If I were to sum up the entire solution in one sentence:

**"Don't just predict revenueâ€”predict what actions will change revenue."**

That's the difference between analytics and intelligence.

---
